### Утилита для рекурсивного извлечения теста по url
Данная утилита обходит url-адреса до заданной глубины начиная с базового и извлекает и сохраняет текст из html, а также pdf документов. Общее число url для которых будет извлекаться текст также можно ограничить. Для каждого url происходит извлечение текста из html-документа, пришедшего через get-запрос по данному url. Минимальное количество символов для сохранения документа равно 300. Извлечённые тексты удовлетворяющие данному условию сохраняются в виде текстовых документов в директории data (по умолчанию).

### Установка
```bash
git clone https://github.com/myPar/UrlContentExtractor
cd UrlContentExtractor
pip install -r requirements.txt
```
Теперь можно использовать:
```bash
python main.py --base_url=https://www.nsu.ru/n/
```
help
```bash
python main.py --help
```

#### Использование
Аргументы:

* __base_url__ - url с которого начинается обход
* __max_urls__ - максимальное число url для которых будет извлекаться текст (50 по умолчанию)
* __depth__ - глубина обхода (2 по умолчанию)
* __reject_http__ - определяет будет ли производится обход для url без https. False - если не будет (по умолчанию)
* __output__ - дирректория куда будут сохраняться документы с извлечённым текстом
* __log__ - логирование (false по умолчанию)
* __ignored_domens__ - список дополнительных доменом, url с которыми будут игнорироваться. Допустим если мы не хотим тянуть html с какого-нибудь youtube, указываем строку **youtube.com**. По умолчанию игнорируются следующие домены: 'vk.com', 't.me', 'rutube.ru', 'dzen.ru', 'youtube.com', '.css', 'zimbra.com', 'youtu.be', 'ok.ru', 'apple.com', 'alfabank.ru'. Для добавление новых указывайте их списком после этого аргумента.

Пример
```bash
python main.py  --base_url=https://www.nsu.ru/n/ --log=True --depth=3 --max_urls=200
```
В результате этой команд будут стягиваться тексты из html-документов начиная с https://www.nsu.ru/n/ и также со всех дочерних url общим количеством до 200 штук и глубиной обхода до 3. Также программа будет логировать все ошибки и полученные url.